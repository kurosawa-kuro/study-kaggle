ここでは、**エラーなく素早くモデルをビルドする**、あるいは**特徴量抽出**・**前処理**の際によく使うPandasの定番テクニックをいくつか紹介します。可視化は省いて、実践的なものにフォーカスします。

---

## 1. 型変換とデータ型チェック

### 型変換（astype）
モデルに入力する際、文字列（object型）の列を数値型にする・カテゴリ型にするなど頻出です。
```python
# object型カラムをカテゴリ型に変換する
df['col_name'] = df['col_name'].astype('category')

# 数値型に変換する
df['col_name'] = pd.to_numeric(df['col_name'], errors='coerce')
```
- `errors='coerce'` は、変換不能な値をNaNにしてくれます（エラー停止を避けたい時に便利）。

### データ型の確認（info）
```python
df.info()
```
- 行数・各カラムのデータ型・欠損数などが素早く確認できます。

---

## 2. 欠損値の補完・除去

### 欠損値の補完（fillna）
```python
# 平均値で補完
df['col_name'] = df['col_name'].fillna(df['col_name'].mean())

# 特定の値で補完
df['col_name'] = df['col_name'].fillna('missing')

# 前の値・後の値で補完
df['col_name'] = df['col_name'].fillna(method='ffill')  # 前の値で埋める
df['col_name'] = df['col_name'].fillna(method='bfill')  # 後の値で埋める
```

### 欠損値の除去（dropna）
```python
# 欠損が含まれる行を全て削除
df = df.dropna()

# 特定の列が欠損の場合のみ削除
df = df.dropna(subset=['col_name'])
```
- カラム数が多いデータでは、一律に行削除するとデータが激減する場合があるため要注意です。

---

## 3. 重複・不要データの処理

### 重複レコードの確認・削除
```python
# 重複の確認
df.duplicated().sum()

# 重複の削除（先に出現したものを残す）
df = df.drop_duplicates()
```

### 不要カラムの削除
```python
df = df.drop(['col_name1', 'col_name2'], axis=1)
```
- 「分析や学習に不要なカラムを素早く落とす」ことでエラー要因を減らします。

---

## 4. 条件による絞り込み・置換

### 条件に合う行の抽出
```python
# 条件を満たす行だけ抽出
df_filtered = df[df['col_name'] > 1000]
```

### 値の置換（replace, map, applymap）
```python
# map：カテゴリ値を数値に置換するなど
mapping_dict = {'Yes': 1, 'No': 0}
df['col_name'] = df['col_name'].map(mapping_dict)

# replace：文字列等の一括変換にも便利
df['col_name'] = df['col_name'].replace({'missing': 'N/A', '?': None})
```
- `map` は1列だけの値変換、`applymap` はDataFrame全体に適用します。

---

## 5. GroupBy や集計（集約）

### groupby + agg
```python
# Ex. Neighborhoodごとの平均販売価格を集計
df.groupby('Neighborhood')['SalePrice'].agg(['mean', 'count', 'median'])
```
- 特定カテゴリ別の統計値を見て、新しい特徴量を作成する際によく使います。

---

## 6. 新しいカラムの生成

### 計算で作る（assign or 単純演算）
```python
# 面積あたりの価格を作る
df['PricePerArea'] = df['SalePrice'] / df['GrLivArea']

# 複数列にわたる演算で特徴量を作る
df['TotalBath'] = df['FullBath'] + 0.5 * df['HalfBath']
```

### apply で柔軟に加工
```python
def custom_transform(x):
    # 自由に加工する例
    return x**2 if x > 0 else 0

df['transformed_col'] = df['some_col'].apply(custom_transform)
```
- `lambda` を使って一行で書くことも多いです。

---

## 7. カテゴリ変数のエンコーディング

### get_dummies（ワンホットエンコーディング）
```python
df = pd.get_dummies(df, columns=['Neighborhood', 'Exterior1st'], drop_first=True)
```
- `drop_first=True` はダミートラップを避けるために使いますが、必要に応じて変更してください。

### Label Encoding
数値ラベル化だけでよい場合（順序があるカテゴリなどに限定的に使うことが多い）
```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['col_name'] = le.fit_transform(df['col_name'])
```
- **注意**: ラベルエンコードされた数字に「意味の強弱」はないので、カテゴリの順序が無いデータに対してはワンホットを使うことが多いです。

---

## 8. 結合（merge, join）や横に列を追加（concat）

### merge
```python
# 左側を基準に右側データを結合（Keyが共通の場合）
df_merged = pd.merge(df_left, df_right, on='key_col', how='left')
```
- たとえば別テーブルにある補足情報を付加し、新しい特徴量を加えたい時に使います。

### concat（行や列の連結）
```python
# 行方向の連結（行を増やす）
df_concat = pd.concat([df_part1, df_part2], axis=0)

# 列方向の連結（列を増やす）
df_concat_col = pd.concat([df_part1, df_part2], axis=1)
```

---

## 9. 型や分布の最終チェックでエラー回避

- **`.describe()`**：数値列の基本統計量（平均、標準偏差、最小値など）を確認
- **`.value_counts()`**：カテゴリ列の分布を確認（レアカテゴリの有無をチェック）
  
これらを使いながら、**欠損・外れ値が想定外に多くないか**を最終確認しておくと、モデルのフィット時のエラーや性能悪化を防ぐうえで役立ちます。

---

## まとめ
- **型変換・欠損補完**を中心に、モデルが要求する形式へ素早く整える。
- **カテゴリ列のエンコーディング**や、**特徴量生成**で追加カラムを作る。
- **describe**, **info**, **value_counts** などで随時チェックしながら進める。

上記のようなPandasの定番テクニックを押さえておくと、エラーなくデータをクリーンにし、モデルのビルドまで速やかに進められます。特にKaggleなどの実践タスクでは、まず最初にデータ型や欠損値に対処しながら、すばやく特徴量を追加・修正してモデルにトライアンドエラーをかける流れが多いです。